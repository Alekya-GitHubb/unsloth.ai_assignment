ğŸ§  LLM Fine-Tuning Experiments with Unsloth

This repository demonstrates multiple end-to-end fine-tuning techniques for open-weights Large Language Models (LLMs) using the Unsloth
 framework.
The experiments progressively explore full fine-tuning, parameter-efficient fine-tuning (LoRA), reinforcement learning, and continued pre-training, each implemented and demonstrated via Google Colab notebooks.

ğŸ§© Colab 1 â€“ Full Fine-Tuning with Smollm2

Notebook: Colab1-Full_Finetuning.ipynb
Objective: Perform full fine-tuning on a small model (smollm2-135M) using open-weights LLM modules.

ğŸ” Highlights

Uses Unsloth BnB-4bit adapter (unsloth/gemma-3-1b-it-unsloth-bnb-4bit).

Demonstrates complete gradient updates (not LoRA-based).

Defines input format, tokenizer, and dataset structure.

Fine-tunes on a chosen chat or coding dataset.

Explains hyperparameters, training flow, and evaluation.

ğŸ§  References

Unsloth Fine-Tuning Guide

Smollm2 Model Info

Medium Reference

âš™ï¸ Colab 2 â€“ LoRA Parameter-Efficient Fine-Tuning

Notebook: colab2-lora_parameter-efficient-fine-tuning.ipynb
Objective: Reuse the same model and dataset as Colab 1 but apply Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning.

ğŸ” Highlights

Converts full fine-tuning to adapter-based (train fewer parameters).

Ideal for resource-limited training setups (Colab T4/A100).

Keeps the base model frozen while training lightweight adapter matrices.

Demonstrates LoRA configuration (r, alpha, dropout).

ğŸ§  References

LoRA Concepts â€“ Unsloth Docs

ğŸ¯ Colab 3 â€“ Reinforcement Learning (RLHF Setup)

Notebook: colab3_Reinforcement_learning.ipynb
Objective: Implement reinforcement learning on top of a fine-tuned LLM using preference datasets.

ğŸ” Highlights

Demonstrates reward-based training with preferred vs rejected responses.

Builds a basic Reward Model + Policy Model structure.

Shows how feedback loops refine generative behavior.

Explains PPO-like reward mechanisms conceptually.

ğŸ§  References

Reinforcement Learning Guide

ğŸ¤– Colab 4 â€“ Reinforcement Learning with GRPO (Reasoning-Enhanced)

Notebook: colab4-reinformcement-learning-with-grpo.ipynb
Objective: Extend reinforcement learning using GRPO (Guided Reinforcement for Prompt Optimization) for reasoning datasets.

ğŸ” Highlights

Uses problemâ€“solution pairs as datasets.

Rewards reasoning-driven answers rather than memorization.

Implements Unsloth GRPO utilities for efficient tuning.

Illustrates model reasoning chain improvements.

ğŸ§  References

GRPO Tutorial

RL-Reasoning Blog

ğŸ“š Colab 5 â€“ Continued Pre-Training (Domain Adaptation)

Notebook: colab5-Continued-pretraining.ipynb
Objective: Perform continued pre-training to teach an existing LLM new language or domain knowledge.

ğŸ” Highlights

Starts from a checkpoint of previously fine-tuned weights.

Uses unsupervised corpus for incremental learning.

Demonstrates Unsloth-based continued pre-training APIs.

Can be used to make a model learn a new language, domain, or style.

ğŸ§  References

Unsloth Continued Pre-Training Guide

Example: Fine-tuning Chatbot for Mental Health

ğŸ§  Additional Notes

You can continue fine-tuning from custom checkpoints generated by earlier notebooks.

All notebooks demonstrate smooth Colab execution flow (no re-initialization conflicts).

Recommended model choices for exploration:

Smollm2 (135M)

TinyLlama

Phi-3 Mini / Medium

Llama 3 (8B)

Gemma 2 (2B / 9B)

Mistral v0.3 / NeMo 12B

Qwen2 (7B)

ğŸ§° Setup Instructions
# 1ï¸âƒ£ Clone the repo
git clone https://github.com/<your-username>/unsloth-finetuning.git
cd unsloth-finetuning

# 2ï¸âƒ£ Open the Colab link or upload notebooks
# Each file runs independently on Google Colab Pro (T4 / A100).

# 3ï¸âƒ£ Dependencies (auto-installed in each notebook)
pip install unsloth torch transformers datasets bitsandbytes accelerate

ğŸ¥ Video Demonstration (Recommended for Submission)

For each Colab:

Explain the goal, model, and dataset.

Walk through key cells (training loop, logs, and inference).

Show results comparison between baseline and fine-tuned models.

Highlight inference via Ollama or Unsloth model export.

ğŸ“œ References

Unsloth Documentation

Medium â€“ Full LORA with Ollama Lightweight Solution

Unsloth Reinforcement Learning Guide

Unsloth GRPO Tutorial
