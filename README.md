ğŸš€ Unsloth LLM Fine-Tuning & Reinforcement Learning Experiments

This repository showcases a complete series of LLM fine-tuning experiments using the ğŸ¦¥ Unsloth framework
, covering everything from full fine-tuning to LoRA, reinforcement learning, and continued pretraining.

Each Colab notebook in this repository demonstrates a distinct methodology for adapting and enhancing open-weights language models such as Smollm2, Llama 3, Gemma 2, Phi-3, and Mistral.


ğŸŒˆ 1. Full Fine-Tuning (Colab 1)
Notebook: ğŸ‘‰ Open Colab 1
Model: smollm2-135M
Technique: Full-parameter fine-tuning using unsloth-bnb-4bit adapters.
ğŸ”¹ Key Points


Uses Unslothâ€™s full fine-tuning pipeline on a small LLM for demonstration.


Explains input formats, tokenization, and dataset preparation.


Includes video demonstration steps for complete workflow explanation.


Flexible across chat, coding, or Q&A datasets.


ğŸ”¹ References


ğŸ“˜ Unsloth Fine-Tuning Guide


ğŸ“– Medium: LORA + Ollama Lightweight Solution



âš™ï¸ 2. LoRA Parameter-Efficient Fine-Tuning (Colab 2)
Notebook: ğŸ‘‰ Open Colab 2
Model: smollm2-135M
Technique: Low-Rank Adaptation (LoRA) for lightweight fine-tuning.
ğŸ”¹ Key Points


Converts full fine-tuning into parameter-efficient training.


Freezes base model weights â€” updates only LoRA adapter matrices.


Reduces GPU memory use by up to 10Ã—.


Configurable parameters: r, alpha, and dropout.


ğŸ”¹ References


ğŸ“˜ Unsloth LoRA Documentation



ğŸ¯ 3. Reinforcement Learning (RLHF Setup) (Colab 3)
Notebook: ğŸ‘‰ Open Colab 3
Technique: Supervised + reward-based Reinforcement Learning.
Goal: Teach the model preference alignment using chosen vs rejected examples.
ğŸ”¹ Key Points


Implements a reward model and policy model setup.


Simulates human feedback loops.


Demonstrates preference scoring and loss optimization.


Visualizes policy updates during reinforcement steps.


ğŸ”¹ References


ğŸ“˜ Unsloth RL Guide



ğŸ§© 4. Reinforcement Learning with GRPO (Colab 4)
Notebook: ğŸ‘‰ Open Colab 4
Technique: GRPO â€“ Guided Reinforcement for Prompt Optimization.
Goal: Enhance reasoning ability using problem-solution datasets.
ğŸ”¹ Key Points


Uses GRPO for improved logical reasoning in responses.


Incorporates chain-of-thought optimization.


Trains the model to generalize and justify its outputs.


Builds upon reinforcement pipeline with custom reward functions.


ğŸ”¹ References


ğŸ“˜ GRPO Tutorial


ğŸ§© Unsloth Blog â€“ RL Reasoning



ğŸ“š 5. Continued Pretraining (Colab 5)
Notebook: ğŸ‘‰ Open Colab 5
Technique: Continued Pretraining / Domain Adaptation.
Goal: Make LLMs learn a new language, style, or domain.
ğŸ”¹ Key Points


Performs unsupervised continued learning on new corpora.


Extends a modelâ€™s knowledge without forgetting previous tasks.


Supports cross-lingual adaptation (e.g., English â†’ Telugu).


Useful for specialized domains (medical, finance, mental health, etc.).


ğŸ”¹ References


ğŸ“˜ Continued Pretraining Guide


ğŸ§  Medium â€“ Mental Health Chatbot Fine-Tuning Example



ğŸ§© Model Families Used
CategoryModels ExploredğŸ¦™ Meta LlamaLlama 3 (8B), Llama 3.1 (8B)ğŸª¶ MistralMistral v0.3 (7B), Mistral NeMo (12B)ğŸ’ GemmaGemma 2 (2B & 9B)ğŸ§® PhiPhi-3 (Mini & Medium)ğŸ¦ QwenQwen2 (7B)ğŸŒ± Tiny ModelsSmollm2 (135M), TinyLlama (1.1B)

âš™ï¸ Setup Instructions
# Clone the repository
git clone https://github.com/<your-username>/unsloth-finetuning.git
cd unsloth-finetuning

# (Optional) Create a virtual environment
python3 -m venv venv
source venv/bin/activate   # Linux/Mac
# or
venv\Scripts\activate     # Windows

# Install core dependencies
pip install -U unsloth transformers datasets bitsandbytes accelerate torch


ğŸ¥ Video Walkthrough (Suggested for Submission)
For each notebook:
1ï¸âƒ£ State the objective (e.g., â€œFine-tuning Smollm2 on chat datasetâ€).
2ï¸âƒ£ Show key code cells and output logs.
3ï¸âƒ£ Explain the parameters and datasets used.
4ï¸âƒ£ Demonstrate inference (before and after fine-tuning).
5ï¸âƒ£ Summarize results with visual or text metrics.

ğŸ“Š Suggested Extensions


ğŸ§© Export fine-tuned models to Ollama for local deployment.


ğŸ” Chain continued pretraining + LoRA for hybrid experiments.


ğŸ¤– Integrate Unsloth + LangChain for RAG use cases.


ğŸ’¬ Develop a mental-health chatbot using fine-tuned Phi-3 or Smollm2.



ğŸ”— Useful Resources


Unsloth Docs


Fine-Tuning Guide


Reinforcement Learning Guide


GRPO Tutorial


Medium â€“ LORA with Ollama Lightweight Solution



