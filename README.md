ğŸ¦¥ Unsloth LLM Fine-Tuning & Reinforcement Learning Experiments
This repository demonstrates multiple LLM fine-tuning pipelines using Unsloth â€” from full fine-tuning and LoRA to reinforcement learning (RLHF & GRPO) and continued pre-training.
All experiments are executed on Google Colab (Pro) using small- to mid-scale open-weights models (Smollm2, Llama 3, Gemma 2, Phi-3, Mistral).

ğŸ“˜ Quick Navigation
#TaskTechniqueColab Link1ï¸âƒ£Full Fine-TuningTrain all parameters on Smollm2 (135M)Open Colab 1 â€“ Full Fine-Tuning2ï¸âƒ£LoRA Fine-TuningParameter-efficient adaptation (LoRA)Open Colab 2 â€“ LoRA Parameter-Efficient Fine-Tuning3ï¸âƒ£Reinforcement LearningPreference-based RLHF setupOpen Colab 3 â€“ Reinforcement Learning4ï¸âƒ£GRPO Reasoning RLGuided Reinforcement for Prompt OptimizationOpen Colab 4 â€“ RL with GRPO5ï¸âƒ£Continued PretrainingDomain/language extensionOpen Colab 5 â€“ Continued Pretraining
(Tip: Replace Alekya-GitHubb/unsloth-finetuning with your repo name if different.)

ğŸ§© Colab 1 â€“ Full Fine-Tuning
Model: Smollm2-135M
Method: Full parameter fine-tuning using 4-bit quantized Unsloth modules.


Train all model weights end-to-end.


Define input format, tokenizer, and dataset layout.


Visualize loss curves and validation accuracy.


ğŸ”— Resources:


Unsloth Fine-Tuning Guide


Medium Article â€“ LORA with Ollama



âš™ï¸ Colab 2 â€“ LoRA Parameter-Efficient Fine-Tuning
Model: Smollm2-135M
Method: Low-Rank Adaptation (LoRA).


Freeze base weights, train only LoRA adapters.


Tune parameters: r, alpha, dropout.


8-10Ã— less VRAM than full fine-tuning.


ğŸ”— Resources:


LoRA Docs â€“ Unsloth



ğŸ¯ Colab 3 â€“ Reinforcement Learning (RLHF)
Goal: Teach LLMs preferences via human-feedback-style signals.


Use a dataset of preferred vs rejected responses.


Implement reward and policy models.


Apply gradient updates with reward optimization.


ğŸ”— Resources:


Unsloth Reinforcement Learning Guide



ğŸ§  Colab 4 â€“ Reinforcement Learning with GRPO
Goal: Improve reasoning and logical coherence using GRPO.


Train on problemâ€“solution datasets.


Reward chain-of-thought explanations.


Evaluate reasoning depth and clarity.


ğŸ”— Resources:


Train Your Own Reasoning Model â€“ GRPO Tutorial


Unsloth Blog â€“ RL Reasoning



ğŸ“š Colab 5 â€“ Continued Pretraining
Goal: Teach LLMs new domains, languages, or styles via unsupervised pretraining.


Extend a checkpointâ€™s knowledge on new corpus.


Ideal for domain-specific models (e.g. medical, legal, mental health).


Supports multi-lingual adaptation (e.g. English â†’ Telugu).


ğŸ”— Resources:


Unsloth Continued Pretraining Docs


Medium â€“ Fine-Tuning Phi-3 for Mental Health Chatbot



ğŸ§¬ Model Families Explored
CategoryModelsğŸ¦™ Meta LlamaLlama 3 (8B), Llama 3.1 (8B)ğŸ’ GemmaGemma 2 (2B & 9B)ğŸª¶ MistralMistral v0.3 (7B), Mistral NeMo (12B)ğŸ§® PhiPhi-3 Mini & MediumğŸ§  Tiny ModelsSmollm2 (135M), TinyLlama (1.1B)ğŸ¦ QwenQwen2 (7B)

âš™ï¸ Setup
# Clone the repo
git clone https://github.com/Alekya-GitHubb/unsloth-finetuning.git
cd unsloth-finetuning

# Install core dependencies
pip install -U unsloth transformers datasets bitsandbytes accelerate torch


ğŸ¥ Video Demonstration Checklist
âœ… Explain each notebookâ€™s objective.
âœ… Walk through dataset and training cells.
âœ… Highlight key metrics (loss, accuracy).
âœ… Demonstrate model inference (before vs after tuning).
âœ… Summarize results in your own voice.

ğŸ”— Official References


Unsloth Docs


Fine-Tuning Guide


Reinforcement Learning Guide


GRPO Tutorial


Medium â€“ Ollama + LORA



ğŸ‘©â€ğŸ’» Author
Alekya Gudise
ğŸ“ MS Software Engineering, San JosÃ© State University
ğŸ’¼ Ex-LTIMindtree QA Engineer | Python Automation | Cisco Infrastructure
ğŸŒ GitHub @Alekya-GitHubb

Would you like me to generate Colab â€œOpen in Colabâ€ badges (colored buttons) for each notebook instead of plain links?
Itâ€™ll make the README even more polished visually.Is this conversation helpful so far?
