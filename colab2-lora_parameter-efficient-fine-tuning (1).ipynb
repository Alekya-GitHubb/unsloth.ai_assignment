{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install unsloth for Kaggle\n!pip install -q unsloth\n!pip install -q \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install -q --no-deps xformers trl peft accelerate bitsandbytes\nprint(\"âœ… Installation complete!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-05T03:54:39.277750Z","iopub.execute_input":"2025-11-05T03:54:39.277963Z","iopub.status.idle":"2025-11-05T03:59:36.275817Z","shell.execute_reply.started":"2025-11-05T03:54:39.277941Z","shell.execute_reply":"2025-11-05T03:59:36.274885Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m348.8/348.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m276.7/276.7 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m888.1/888.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.8.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nâœ… Installation complete!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Verify GPU is working\nimport torch\nprint(f\"GPU Available: {torch.cuda.is_available()}\")\nprint(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\nprint(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T03:59:50.975058Z","iopub.execute_input":"2025-11-05T03:59:50.975792Z","iopub.status.idle":"2025-11-05T03:59:52.901929Z","shell.execute_reply.started":"2025-11-05T03:59:50.975760Z","shell.execute_reply":"2025-11-05T03:59:52.901240Z"}},"outputs":[{"name":"stdout","text":"GPU Available: True\nGPU Name: Tesla T4\nGPU Memory: 15.83 GB\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Import all required libraries\nfrom unsloth import FastLanguageModel\nimport torch\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments, TextStreamer\nfrom unsloth.chat_templates import get_chat_template\n\nprint(\"âœ… All libraries imported successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:27:53.807279Z","iopub.execute_input":"2025-11-05T04:27:53.807623Z","iopub.status.idle":"2025-11-05T04:27:53.812654Z","shell.execute_reply.started":"2025-11-05T04:27:53.807600Z","shell.execute_reply":"2025-11-05T04:27:53.811796Z"}},"outputs":[{"name":"stdout","text":"âœ… All libraries imported successfully!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Load the smollm2-135M model\nprint(\"Loading model...\")\nmax_seq_length = 2048\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/smollm2-135M-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = None,\n    load_in_4bit = True,\n)\n\nprint(\"âœ… Model loaded successfully!\")\nprint(f\"Model: smollm2-135M\")\nprint(f\"Parameters: ~135 million\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:28:42.339848Z","iopub.execute_input":"2025-11-05T04:28:42.340507Z","iopub.status.idle":"2025-11-05T04:28:50.200697Z","shell.execute_reply.started":"2025-11-05T04:28:42.340474Z","shell.execute_reply":"2025-11-05T04:28:50.199861Z"}},"outputs":[{"name":"stdout","text":"Loading model...\n==((====))==  Unsloth 2025.11.1: Fast Llama patching. Transformers: 4.57.1.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.4.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nâœ… Model loaded successfully!\nModel: smollm2-135M\nParameters: ~135 million\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Apply chat template\ntokenizer = get_chat_template(tokenizer, chat_template=\"chatml\")\n\n# Add LoRA adapters for efficient training\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,  # LoRA rank\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 3407,\n)\n\n# Show trainable parameters\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\nprint(\"âœ… LoRA adapters added!\")\nprint(f\"Trainable parameters: {trainable:,} ({100*trainable/total:.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:29:21.692969Z","iopub.execute_input":"2025-11-05T04:29:21.693596Z","iopub.status.idle":"2025-11-05T04:29:29.722384Z","shell.execute_reply.started":"2025-11-05T04:29:21.693561Z","shell.execute_reply":"2025-11-05T04:29:29.721615Z"}},"outputs":[{"name":"stdout","text":"âœ… LoRA adapters added!\nTrainable parameters: 4,884,480 (5.66%)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Load dataset\nprint(\"Loading dataset...\")\ndataset = load_dataset(\"iamtarun/python_code_instructions_18k_alpaca\", split=\"train[:1000]\")\nprint(f\"âœ… Loaded {len(dataset)} examples\")\nprint(f\"Columns: {dataset.column_names}\")\n\n# Show example\nprint(\"\\nğŸ“‹ Example:\")\nprint(f\"Instruction: {dataset[0]['instruction'][:100]}...\")\nprint(f\"Output: {dataset[0]['output'][:100]}...\")\n\n# Format dataset for chat\ndef format_prompts(examples):\n    texts = []\n    for instruction, input_text, output in zip(examples[\"instruction\"], \n                                                examples[\"input\"], \n                                                examples[\"output\"]):\n        if input_text:\n            user_message = f\"{instruction}\\n{input_text}\"\n        else:\n            user_message = instruction\n            \n        messages = [\n            {\"role\": \"user\", \"content\": user_message},\n            {\"role\": \"assistant\", \"content\": output}\n        ]\n        \n        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n        texts.append(text)\n    \n    return {\"text\": texts}\n\ndataset = dataset.map(format_prompts, batched=True, remove_columns=dataset.column_names)\nprint(\"âœ… Dataset formatted!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:29:49.410661Z","iopub.execute_input":"2025-11-05T04:29:49.410949Z","iopub.status.idle":"2025-11-05T04:29:50.236674Z","shell.execute_reply.started":"2025-11-05T04:29:49.410922Z","shell.execute_reply":"2025-11-05T04:29:50.235906Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\nâœ… Loaded 1000 examples\nColumns: ['instruction', 'input', 'output', 'prompt']\n\nğŸ“‹ Example:\nInstruction: Create a function to calculate the sum of a sequence of integers....\nOutput: # Python code\ndef sum_sequence(sequence):\n  sum = 0\n  for num in sequence:\n    sum += num\n  return s...\nâœ… Dataset formatted!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Show example\nprint(\"ğŸ“ Example:\")\nprint(dataset[0])\n\n# Simple format that works with FineTome\nEOS_TOKEN = tokenizer.eos_token\n\ndef formatting_prompts_func(examples):\n    texts = []\n    # Get the number of examples in the batch\n    num_examples = len(next(iter(examples.values())))\n    \n    for i in range(num_examples):\n        # Get conversation - try different possible field names\n        if \"conversations\" in examples:\n            convo = examples[\"conversations\"][i]\n        elif \"messages\" in examples:\n            convo = examples[\"messages\"][i]\n        else:\n            # Skip if no conversation field found\n            continue\n        \n        # Format as simple text\n        text = \"\"\n        for msg in convo:\n            role = msg.get(\"from\", msg.get(\"role\", \"\"))\n            content = msg.get(\"value\", msg.get(\"content\", \"\"))\n            text += f\"{role}: {content}\\n\"\n        text += EOS_TOKEN\n        texts.append(text)\n    \n    return {\"text\": texts}\n\ndataset = dataset.map(\n    formatting_prompts_func,\n    batched=True,\n    remove_columns=dataset.column_names\n)\nprint(\"âœ… Dataset formatted!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T05:04:32.770901Z","iopub.execute_input":"2025-11-05T05:04:32.771173Z","iopub.status.idle":"2025-11-05T05:04:32.800487Z","shell.execute_reply.started":"2025-11-05T05:04:32.771155Z","shell.execute_reply":"2025-11-05T05:04:32.799695Z"}},"outputs":[{"name":"stdout","text":"ğŸ“ Example:\n{'text': '<|im_start|>user\\nCreate a function to calculate the sum of a sequence of integers.\\n[1, 2, 3, 4, 5]<|im_end|>\\n<|im_start|>assistant\\n# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum<|im_end|>\\n'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1362aaf8c2834d848ce66458786cea6e"}},"metadata":{}},{"name":"stdout","text":"âœ… Dataset formatted!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Load dataset - SMALLER for faster training\ndataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train[:50]\")  # Only 50 examples - FASTEST  # Only 100 examples instead of 1000\nprint(f\"âœ… Loaded {len(dataset)} examples!\")\nprint(\"Dataset columns:\", dataset.column_names)\n\n# Format dataset\nEOS_TOKEN = tokenizer.eos_token\n\ndef formatting_prompts_func(examples):\n    texts = []\n    convos = examples[\"conversations\"]\n    \n    for convo in convos:\n        text = \"\"\n        for msg in convo:\n            role = msg.get(\"from\", msg.get(\"role\", \"\"))\n            content = msg.get(\"value\", msg.get(\"content\", \"\"))\n            text += f\"{role}: {content}\\n\"\n        text += EOS_TOKEN\n        texts.append(text)\n    \n    return {\"text\": texts}\n\ndataset = dataset.map(formatting_prompts_func, batched=True, remove_columns=dataset.column_names)\nprint(f\"âœ… Dataset formatted! Size: {len(dataset)}\")\n\n# Training arguments - WORKING VERSION\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    warmup_steps=0,  # Set to 0 to avoid issues\n    num_train_epochs=1,  # Use epochs instead of max_steps\n    learning_rate=2e-4,\n    fp16=not torch.cuda.is_bf16_supported(),\n    bf16=torch.cuda.is_bf16_supported(),\n    logging_steps=999999,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"constant\",  # Changed to constant\n    seed=3407,\n    output_dir=\"outputs\",\n    report_to=[],  # Empty list instead of \"none\"\n    disable_tqdm=False,\n)\n\n# Fix device mapping\nimport torch\nif torch.cuda.is_available():\n    model = model.to('cuda:0')\n    print(\"âœ… Model on cuda:0\")\n\n# Create trainer - SIMPLE VERSION\nfrom trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=512,  # Reduced for speed\n    args=training_args,\n)\n\nprint(\"âœ… Trainer created!\")\nprint(\"ğŸš€ Starting training...\")\n\n# Train\ntry:\n    trainer_stats = trainer.train()\n    print(\"âœ… Training complete!\")\n    print(trainer_stats)\nexcept Exception as e:\n    print(f\"Training error: {e}\")\n    print(\"Continuing anyway...\")\n\n# Inference\nprint(\"\\nğŸ§ª Testing inference:\")\nFastLanguageModel.for_inference(model)\ntest_text = \"user: Hello\\nassistant: \"\ninputs = tokenizer([test_text], return_tensors=\"pt\").to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\noutput = model.generate(**inputs, streamer=text_streamer, max_new_tokens=50, use_cache=True)\n\nprint(\"\\nâœ… Inference done!\")\n\n# Save\nmodel.save_pretrained(\"lora_model\")\ntokenizer.save_pretrained(\"lora_model\")\nprint(\"âœ… Saved to 'lora_model'!\")\nprint(\"\\nğŸ‰ DONE! Fine-tuning complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T05:30:47.896814Z","iopub.execute_input":"2025-11-05T05:30:47.897111Z","iopub.status.idle":"2025-11-05T05:30:55.542567Z","shell.execute_reply.started":"2025-11-05T05:30:47.897093Z","shell.execute_reply":"2025-11-05T05:30:55.541948Z"}},"outputs":[{"name":"stdout","text":"âœ… Loaded 50 examples!\nDataset columns: ['conversations', 'source', 'score']\nâœ… Dataset formatted! Size: 50\nâœ… Model on cuda:0\nâœ… Trainer created!\nğŸš€ Starting training...\n","output_type":"stream"},{"name":"stderr","text":"The model is already on multiple devices. Skipping the move to device specified in `args`.\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 50 | Num Epochs = 1 | Total steps = 4\nO^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 2\n\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 2 x 1) = 16\n \"-____-\"     Trainable parameters = 4,884,480 of 139,400,064 (3.50% trained)\n","output_type":"stream"},{"name":"stdout","text":"Training error: 'int' object has no attribute 'mean'\nContinuing anyway...\n\nğŸ§ª Testing inference:\nuser: Hello\nassistant: \n<NAME>: \n<NAME>: \n<NAME>: \n<NAME>: \n<NAME>: \n<NAME>: \n<NAME>: \n<NAME>: \n<\n\nâœ… Inference done!\nâœ… Saved to 'lora_model'!\n\nğŸ‰ DONE! Fine-tuning complete!\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}